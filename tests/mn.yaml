apiVersion: elastic.pytorch.org/v1alpha1
kind: ElasticJob
metadata:
  name: test
  namespace: elastic-job
spec:
  # Use "etcd-service:2379" if you already apply etcd.yaml
  rdzvEndpoint: etcd://10.100.232.130:2379/test?min_workers=2&max_workers=2
  minReplicas: 2
  maxReplicas: 2
  replicaSpecs:
    Worker:
      replicas: 2
      restartPolicy: ExitCode
      template:
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: test
              image: pytorchlightning/pytorch_lightning:base-cuda-py3.7-torch1.8
              imagePullPolicy: Always
              command:
              - bash
              - -ce
              - |
                mkdir -p /home/runner/work/pytorch-lightning && cd /home/runner/work/pytorch-lightning
                git clone https://github.com/PyTorchLightning/pytorch-lightning.git
                cd pytorch-lightning
                echo $PWD
                git fetch --all
                git checkout %(SHA)s
                pip install -e .
                pip install torchelastic
                export PL_RUNNING_SPECIAL_TESTS=1
                DEFAULTS_ELASTIC=" -m torchelastic.distributed.launch --nnodes=2 --nproc_per_node=1 --rdzv_id=test --rdzv_backend=etcd --rdzv_endpoint=10.100.232.130:2379"
                DEFAULTS_FIRST=" -m coverage run --source pytorch_lightning -m pytest --verbose --capture=no"
                DEFAULTS_AFTER=" -m coverage run --source pytorch_lightning -a -m pytest --verbose --capture=no"
                # special tests list

                python $DEFAULTS_ELASTIC $DEFAULTS_FIRST ./tests/backends/test_multi_nodes_gpu.py::test_logging_sync_dist_true_ddp
                python $DEFAULTS_ELASTIC $DEFAULTS_AFTER ./tests/backends/test_multi_nodes_gpu.py::test_logging_sync_dist_true_ddp

                coverage report
                # special tests list
                echo "END_TOKEN"
                cat .coverage | base64
              resources:
                limits:
                  nvidia.com/gpu: 2
